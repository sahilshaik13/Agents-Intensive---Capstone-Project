{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† InsightGraph: The Gemini-Powered Multi-Agent Researcher\n",
    "\n",
    "## üöÄ Project Overview\n",
    "\n",
    "**Problem:** Analyzing dense scientific papers and technical documents is time-consuming. Researchers often struggle to quickly extract core concepts and visualize the relationships between complex entities without manually reading every page.\n",
    "\n",
    "**Solution:** This **Multi-Agent System** automates the research process using a single, self-contained Jupyter Notebook. It employs a team of specialized AI agents working sequentially to ingest data, memorize context, summarize findings, and programmatically generate knowledge graphs.\n",
    "\n",
    "Unlike standard chatbots, this system uses **active tooling**‚Äîthe agents don't just \"talk\"; they write and execute their own Python code to create visualizations on the fly.\n",
    "\n",
    "-----\n",
    "\n",
    "## ‚ú® Key Features (Hackathon Criteria Implemented)\n",
    "\n",
    "This project demonstrates mastery of the following advanced agentic concepts:\n",
    "\n",
    "### 1\\. ü§ñ Multi-Agent Architecture\n",
    "\n",
    "We utilize a **Sequential Agent Workflow** with two distinct roles:\n",
    "\n",
    "  * **`Curie` (Senior Researcher):** Specialized in data ingestion, context compaction, and high-level summarization.\n",
    "  * **`DaVinci` (Data Visualizer):** Specialized in translating textual insights into executable Python code (`NetworkX`/`Matplotlib`) for visualization.\n",
    "\n",
    "### 2\\. üß† Long-Term Memory & RAG\n",
    "\n",
    "  * **Memory Bank:** Implements a persistent **Vector Store** using `FAISS` and `SentenceTransformers` directly within the notebook session.\n",
    "  * **Context Engineering:** The system chunks PDF data, embeds it, and allows agents to query the memory bank for specific details (RAG) rather than overloading the context window.\n",
    "\n",
    "### 3\\. üõ†Ô∏è Custom Tools & Code Execution\n",
    "\n",
    "  * **PDF Ingestion Tool:** A custom tool to parse, clean, and extract text from PDF documents.\n",
    "  * **Dynamic Code Execution:** The `VisualizerAgent` writes raw Python code, which is then safely extracted using Regex and executed by the system to render knowledge graphs immediately in the notebook output.\n",
    "\n",
    "### 4\\. üëÅÔ∏è Observability & Resilience\n",
    "\n",
    "  * **Agent Logger:** Custom color-coded logging provides real-time tracing of agent thoughts, tool inputs, and system status directly in the cell outputs.\n",
    "  * **Production-Grade Retry Logic:** Implements **Exponential Backoff** to handle API rate limits gracefully, ensuring the agent recovers automatically from `ResourceExhausted` errors.\n",
    "  * **Safe Mode Orchestrator:** Intelligent delays between agent hand-offs to ensure stability on the Gemini Free Tier.\n",
    "\n",
    "-----\n",
    "\n",
    "## üèóÔ∏è System Architecture\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    User[User Input (PDF)] -->|Step 1| ResAgent[üïµÔ∏è Research Agent]\n",
    "    \n",
    "    subgraph \"Memory System\"\n",
    "        ResAgent -->|Ingest| VectorDB[(FAISS Memory Bank)]\n",
    "        VectorDB -->|Query Context| ResAgent\n",
    "    end\n",
    "    \n",
    "    subgraph \"Analysis Phase\"\n",
    "        ResAgent -->|Summary & Entities| VisAgent[üé® Visualizer Agent]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Execution Phase\"\n",
    "        VisAgent -->|Generates Python Code| ExecTool[‚öôÔ∏è Code Executor]\n",
    "        ExecTool -->|Renders| Output[üìä Knowledge Graph]\n",
    "    end\n",
    "```\n",
    "\n",
    "[Image of multi agent system architecture diagram]\n",
    "\n",
    "-----\n",
    "\n",
    "## üíª Setup & Usage\n",
    "\n",
    "This project is designed to be \"Plug and Play\" within a single Jupyter Notebook.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "  * A Google Gemini API Key.\n",
    "  * An environment that runs Jupyter Notebooks (e.g., Google Colab, Kaggle, or local JupyterLab).\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1.  **Open the Notebook:**\n",
    "2.  **Configure API Key:**\n",
    "      * Create a `.env` file in the same directory with `GEMINI_API_KEY=your_key`, OR\n",
    "      * Directly paste your key into the `API_KEY` variable in **Cell 2**.\n",
    "3.  **Run All Cells:**\n",
    "      * **Cell 1** will automatically install all necessary dependencies (`google-generativeai`, `faiss-cpu`, `pdfplumber`, etc.).\n",
    "      * The subsequent cells will initialize the Agents, Memory Bank, and Tool Box.\n",
    "4.  **Watch the Magic:** The final cell executes the `main_system()` function. You will see color-coded logs indicating which agent is thinking, followed by the generation of a Knowledge Graph visualization.\n",
    "\n",
    "-----\n",
    "\n",
    "## üõ°Ô∏è Robustness & Error Handling\n",
    "\n",
    "This agent is built for reliability:\n",
    "\n",
    "  * **Rate Limit Handling:** If the Gemini API returns a `ResourceExhausted` error, the agent automatically pauses and retries with increasing delays (Exponential Backoff).\n",
    "  * **Syntax Correction:** The tool execution layer uses Regex to surgically extract code blocks from LLM responses, preventing \"conversational filler\" syntax errors during code execution.\n",
    "\n",
    "-----\n",
    "\n",
    "## üîÆ Future Improvements\n",
    "\n",
    "  * **Web Search Tool:** Granting the Research Agent access to Google Search to validate claims found in the PDF.\n",
    "  * **Interactive UI:** Wrapping the notebook logic into a Streamlit app for a drag-and-drop PDF experience.\n",
    "  * **Docker Container:** Containerizing the environment for enhanced security during arbitrary code execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find platform independent libraries <prefix>\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfplumber sentence-transformers faiss-cpu matplotlib networkx python-dotenv google-generativeai colorama --quiet\n",
    "print(\"Dependencies installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import pdfplumber\n",
    "import faiss\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import google.generativeai as genai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any, Callable\n",
    "from colorama import Fore, Style\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import time\n",
    "from google.api_core import exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Gemini API Key: AIzaSyD1****\n"
     ]
    }
   ],
   "source": [
    "    # ===== 1. OBSERVABILITY & CONFIG (Criteria: Observability) =====\n",
    "load_dotenv()\n",
    "# Ensure you have your key in a .env file or set it here for the notebook\n",
    "# os.environ[\"GEMINI_API_KEY\"] = \"YOUR_KEY_HERE\" \n",
    "API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "genai.configure(api_key=API_KEY)\n",
    "print(f\"Loaded Gemini API Key: {API_KEY[:8] if API_KEY else 'None'}****\")\n",
    "\n",
    "#Testing the API Key\n",
    "from google.generativeai import GenerativeModel\n",
    "\n",
    "model = GenerativeModel('gemini-3-pro-preview')\n",
    "response = model.generate_content(\"Hello Gemini, test my quota.\")\n",
    "print(response.text)\n",
    "\n",
    "# Custom Logger for \"Tracing\"\n",
    "class AgentLogger:\n",
    "    @staticmethod\n",
    "    def info(agent_name, message):\n",
    "        print(f\"{Fore.CYAN}[{agent_name}]{Style.RESET_ALL} {message}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def tool(tool_name, input_data):\n",
    "        print(f\"{Fore.YELLOW}  ‚îî‚îÄ‚îÄ [Tool: {tool_name}]{Style.RESET_ALL} Input: {str(input_data)[:50]}...\")\n",
    "\n",
    "    @staticmethod\n",
    "    def success(message):\n",
    "        print(f\"{Fore.GREEN}‚úî {message}{Style.RESET_ALL}\")\n",
    "\n",
    "logger = AgentLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===== 2. MEMORY BANK (Criteria: Long Term Memory) =====\n",
    "class MemoryBank:\n",
    "    def __init__(self):\n",
    "        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.index = None\n",
    "        self.documents = [] # Maps index ID to actual text chunk\n",
    "\n",
    "    def add_memory(self, text_chunks: List[str]):\n",
    "        if not text_chunks: return\n",
    "        embeddings = self.encoder.encode(text_chunks)\n",
    "        dimension = embeddings.shape[1]\n",
    "        \n",
    "        if self.index is None:\n",
    "            self.index = faiss.IndexFlatL2(dimension)\n",
    "        \n",
    "        self.index.add(np.array(embeddings))\n",
    "        self.documents.extend(text_chunks)\n",
    "        logger.success(f\"Added {len(text_chunks)} chunks to Long Term Memory (FAISS)\")\n",
    "\n",
    "    def query_memory(self, query: str, k: int = 3) -> str:\n",
    "        if self.index is None: return \"\"\n",
    "        vec = self.encoder.encode([query])\n",
    "        distances, indices = self.index.search(vec, k)\n",
    "        \n",
    "        results = [self.documents[i] for i in indices[0] if i < len(self.documents)]\n",
    "        return \"\\n\".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 3. TOOLS (Enhanced Debugging) =====\n",
    "class ToolBox:\n",
    "    @staticmethod\n",
    "    def read_pdf(pdf_path: str) -> str:\n",
    "        logger.tool(\"read_pdf\", pdf_path)\n",
    "        text = \"\"\n",
    "        try:\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    extract = page.extract_text()\n",
    "                    if extract: text += extract\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            return f\"Error reading PDF: {e}\"\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_keywords_generated(text: str) -> List[str]:\n",
    "        words = re.findall(r'\\b[A-Z][a-zA-Z]{4,}\\b', text)\n",
    "        return list(set(words))[:15]\n",
    "\n",
    "    @staticmethod\n",
    "    def execute_python_plot(code: str):\n",
    "        logger.tool(\"execute_python_plot\", \"Executing generated code...\")\n",
    "        try:\n",
    "            # We inject local variables so the code can see them\n",
    "            local_scope = {'nx': nx, 'plt': plt}\n",
    "            exec(code, globals(), local_scope)\n",
    "            logger.success(\"Graph generated successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"{Fore.RED}Execution Error: {e}{Style.RESET_ALL}\")\n",
    "            print(f\"{Fore.RED}--- Failed Code Preview ---{Style.RESET_ALL}\")\n",
    "            print(code[:300] + \"...\") # Print start of code to see what went wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===== 4. AGENT ARCHITECTURE (Criteria: Agent powered by LLM) =====\n",
    "\n",
    "class BaseAgent:\n",
    "    def __init__(self, name: str, role: str, model_name=\"gemini-3-pro-preview\"):\n",
    "        self.name = name\n",
    "        self.role = role\n",
    "        self.model = genai.GenerativeModel(model_name)\n",
    "        self.history = [] # Session Memory\n",
    "\n",
    "    def think(self, prompt: str, context: str = \"\") -> str:\n",
    "        logger.info(self.name, f\"Thinking about: {prompt[:30]}...\")\n",
    "        full_prompt = f\"\"\"\n",
    "        You are {self.name}, a {self.role}.\n",
    "        \n",
    "        Context from Memory/Tools:\n",
    "        {context}\n",
    "        \n",
    "        Task: {prompt}\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- RETRY LOGIC (Exponential Backoff) ---\n",
    "        max_retries = 5\n",
    "        base_delay = 5  # Start with 5 seconds wait\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.model.generate_content(full_prompt)\n",
    "                self.history.append({\"role\": \"user\", \"content\": prompt})\n",
    "                self.history.append({\"role\": \"assistant\", \"content\": response.text})\n",
    "                return response.text\n",
    "            \n",
    "            except exceptions.ResourceExhausted:\n",
    "                wait_time = base_delay * (2 ** attempt) # 5s, 10s, 20s, 40s...\n",
    "                print(f\"{Fore.RED}‚ö†Ô∏è Quota limit hit. Pausing for {wait_time} seconds... (Attempt {attempt+1}/{max_retries}){Style.RESET_ALL}\")\n",
    "                time.sleep(wait_time)\n",
    "            \n",
    "            except Exception as e:\n",
    "                # Catch other random errors\n",
    "                print(f\"{Fore.RED}An error occurred: {e}{Style.RESET_ALL}\")\n",
    "                return \"I encountered an error and could not think.\"\n",
    "\n",
    "        return \"Error: Failed to generate response after maximum retries.\"\n",
    "\n",
    "# --- Specific Agents (Keep these the same) ---\n",
    "\n",
    "class ResearchAgent(BaseAgent):\n",
    "    \"\"\"Responsible for reading data and storing it in memory.\"\"\"\n",
    "    def ingest_knowledge(self, filepath: str, memory_bank: MemoryBank):\n",
    "        raw_text = ToolBox.read_pdf(filepath)\n",
    "        # Context Compaction: Chunking text for vector store\n",
    "        chunks = [raw_text[i:i+500] for i in range(0, len(raw_text), 500)]\n",
    "        memory_bank.add_memory(chunks)\n",
    "        return raw_text[:2000] # Return summary context\n",
    "\n",
    "class VisualizerAgent(BaseAgent):\n",
    "    \"\"\"Responsible for writing Python code to visualize data.\"\"\"\n",
    "    def generate_graph_code(self, summary: str, entities: List[str]) -> str:\n",
    "        prompt = f\"\"\"\n",
    "        Write Python code using NetworkX and Matplotlib to visualize a Knowledge Graph.\n",
    "        \n",
    "        The Central Node should be: \"Paper Summary\"\n",
    "        The Connected Nodes should be: {', '.join(entities)}\n",
    "        \n",
    "        Requirements:\n",
    "        1. Use a spring layout.\n",
    "        2. Make the central node red and large.\n",
    "        3. Make connected nodes skyblue and smaller.\n",
    "        4. Draw edges with gray color.\n",
    "        5. DO NOT include plt.show(), just setup the plot.\n",
    "        6. Wrap code in ```python blocks.\n",
    "        \"\"\"\n",
    "        response = self.think(prompt, context=f\"Summary of paper: {summary[:300]}...\")\n",
    "        \n",
    "        # Robust Regex Extraction\n",
    "        match = re.search(r\"```python(.*?)```\", response, re.DOTALL)\n",
    "        if match:\n",
    "            code = match.group(1).strip()\n",
    "        else:\n",
    "            code = response.replace(\"```python\", \"\").replace(\"```\", \"\").strip()\n",
    "            \n",
    "        return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m=== STARTING MULTI-AGENT RESEARCH SYSTEM (SAFE MODE) ===\u001b[0m\n",
      "\n",
      "\u001b[36m[Orchestrator]\u001b[0m Dispatching Researcher...\n",
      "\u001b[33m  ‚îî‚îÄ‚îÄ [Tool: read_pdf]\u001b[0m Input: bitcoin.pdf...\n",
      "\u001b[32m‚úî Added 43 chunks to Long Term Memory (FAISS)\u001b[0m\n",
      "\u001b[33m‚è≥ Cooling down API for 5 seconds...\u001b[0m\n",
      "\u001b[36m[Curie]\u001b[0m Thinking about: Summarize the scientific core ...\n",
      "\u001b[31m‚ö†Ô∏è Quota limit hit. Pausing for 5 seconds... (Attempt 1/5)\u001b[0m\n",
      "\u001b[31m‚ö†Ô∏è Quota limit hit. Pausing for 10 seconds... (Attempt 2/5)\u001b[0m\n",
      "\u001b[31m‚ö†Ô∏è Quota limit hit. Pausing for 20 seconds... (Attempt 3/5)\u001b[0m\n",
      "\u001b[31m‚ö†Ô∏è Quota limit hit. Pausing for 40 seconds... (Attempt 4/5)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ===== 5. MAIN ORCHESTRATOR (Safe Mode) =====\n",
    "def main_system(pdf_path):\n",
    "    print(f\"{Fore.MAGENTA}=== STARTING MULTI-AGENT RESEARCH SYSTEM (SAFE MODE) ==={Style.RESET_ALL}\\n\")\n",
    "    \n",
    "    # Initialize Infrastructure\n",
    "    memory = MemoryBank()\n",
    "    \n",
    "    # Initialize Agents\n",
    "    researcher = ResearchAgent(\"Curie\", \"Senior Researcher\")\n",
    "    visualizer = VisualizerAgent(\"DaVinci\", \"Data Visualizer\")\n",
    "    \n",
    "    # --- Step 1: Researcher Action ---\n",
    "    logger.info(\"Orchestrator\", \"Dispatching Researcher...\")\n",
    "    initial_text = researcher.ingest_knowledge(pdf_path, memory)\n",
    "    \n",
    "    # --- SAFETY PAUSE 1 ---\n",
    "    # The API free tier can be sensitive to bursts. We wait 5 seconds.\n",
    "    print(f\"{Fore.YELLOW}‚è≥ Cooling down API for 5 seconds...{Style.RESET_ALL}\")\n",
    "    time.sleep(5) \n",
    "    \n",
    "    # --- Step 2: Use Gemini for Analysis ---\n",
    "    # Retrieve context from the Long Term Memory\n",
    "    context = memory.query_memory(\"Main concepts and conclusions\")\n",
    "    \n",
    "    # This is the first API Call (Researcher)\n",
    "    summary = researcher.think(\"Summarize the scientific core of this text in 3 sentences.\", context=context)\n",
    "    print(f\"\\n{Fore.WHITE}{Style.DIM}--- Generated Summary ---\\n{summary}{Style.RESET_ALL}\\n\")\n",
    "    \n",
    "    # --- Step 3: Tool Extraction ---\n",
    "    # Extract entities using the ToolBox\n",
    "    entities = ToolBox.extract_keywords_generated(initial_text)\n",
    "    logger.success(f\"Extracted Entities: {entities}\")\n",
    "    \n",
    "    # --- SAFETY PAUSE 2 ---\n",
    "    # Crucial: Wait before the next agent starts to reset the Rate Limit counter\n",
    "    print(f\"{Fore.YELLOW}‚è≥ Handing off to Visualizer (Waiting 10s to respect Rate Limits)...{Style.RESET_ALL}\")\n",
    "    time.sleep(10)\n",
    "    \n",
    "    # --- Step 4: Visualizer Action ---\n",
    "    logger.info(\"Orchestrator\", \"Dispatching Visualizer...\")\n",
    "    \n",
    "    # This is the second API Call (Visualizer)\n",
    "    graph_code = visualizer.generate_graph_code(summary, entities)\n",
    "    \n",
    "    # --- Step 5: Code Execution Tool ---\n",
    "    # Execute the Python code generated by the agent\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    ToolBox.execute_python_plot(graph_code)\n",
    "    plt.title(\"Agent-Generated Knowledge Graph\", fontsize=15)\n",
    "    plt.show()\n",
    "\n",
    "# Run the system\n",
    "if __name__ == \"__main__\":\n",
    "    # Check if the PDF exists; if not, download a sample (Bitcoin Whitepaper)\n",
    "    if not os.path.exists(\"bitcoin.pdf\"):\n",
    "        logger.info(\"System\", \"Downloading sample bitcoin.pdf...\")\n",
    "        r = requests.get(\"https://bitcoinpaper.org/bitcoin.pdf\")\n",
    "        with open(\"bitcoin.pdf\", \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "            \n",
    "    # Execute the main orchestrator\n",
    "    main_system(\"bitcoin.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
